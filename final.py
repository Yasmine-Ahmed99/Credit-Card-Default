# -*- coding: utf-8 -*-
"""final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P-J-xOALrZ7tte8aoWlEF1vkPf9u3b7e
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np 
import pandas as pd 
from pandas import Timestamp
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

df = pd.read_csv('/content/UCI_Credit_Card.csv')
df

print(df.shape)

count_nan = df.isna().sum().sum()

print ('Count of NaN: ' + str(count_nan))

print(df.shape)

"""#Preprocessing and Cleaning """

df.info()

#rename some colums 
df = df.rename(columns={'default.payment.next.month': 'default_payment', 
                        'PAY_0': 'PAY_1'})

sns.countplot(df['default_payment'])

"""Categorical variables"""

df['LIMIT_BAL']

value = df['LIMIT_BAL'].quantile(0.98)
df = df.replace(np.inf, value)

df.describe()

df['SEX'].value_counts()

df['MARRIAGE'].value_counts()

df['SEX'].value_counts().plot(kind = 'bar')

df['MARRIAGE'].value_counts().plot(kind = 'bar')

"""Numerical variables"""

def draw_histogram(df, variables, nun_rows, num_cols):
    fig=plt.figure()
    for i, n in enumerate(variables):
        ax=fig.add_subplot(nun_rows,num_cols,i+1)
        df[n].hist(bins=10,ax=ax)
        ax.set_title(n)
    fig.tight_layout()  
    plt.show()

payment =  df[['PAY_AMT1','PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']]

draw_histogram(payment, payment.columns, 2, 3)

bill = df[['BILL_AMT1','BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6']]

draw_histogram(bill,bill,2,3)

notpay = df[['PAY_1','PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']]
draw_histogram(notpay, notpay.columns, 2, 3)

# fig, ax = plt.subplots(1, 2, figsize=(15,4))
# sns.histplot(data=df, x='SEX', ax=ax[0])
# ax[0].set_title("Histogram")
# sns.boxplot(data=df, x='SEX', ax=ax[1])
# ax[1].set_title("Boxplot");

df.columns

#calculte the correlation between columns
def correlation(Column1, Column2):
    result = df.groupby([Column1, Column2]).size().unstack()
    result['perc'] = (result[result.columns[1]]/(result[result.columns[0]] + result[result.columns[1]]))
    return result

correlation('SEX','default_payment')

correlation('MARRIAGE','default_payment')

correlation('EDUCATION','default_payment')

df.loc[df['MARRIAGE'] == 0, 'MARRIAGE'] = 3
df['MARRIAGE'].value_counts()

correlation('MARRIAGE','default_payment')

correlation('EDUCATION', 'SEX')

correlation('MARRIAGE', 'SEX')

fill = (df['PAY_1'] == -2) | (df['PAY_1'] == -1) | (df['PAY_1'] == 0)
df.loc[fill,'PAY_1'] = 0
fill = (df['PAY_2'] == -2) | (df['PAY_2'] == -1) | (df['PAY_2'] == 0)
df.loc[fill, 'PAY_2'] = 0
fill = (df['PAY_3'] == -2) | (df['PAY_3'] == -1) | (df['PAY_3'] == 0)
df.loc[fill, 'PAY_3'] = 0
fill = (df['PAY_4'] == -2) | (df['PAY_4'] == -1) | (df['PAY_4'] == 0)
df.loc[fill, 'PAY_4'] = 0
fill = (df['PAY_5'] == -2) | (df['PAY_5'] == -1) | (df['PAY_5'] == 0)
df.loc[fill, 'PAY_5'] = 0
fill = (df['PAY_6'] == -2) | (df['PAY_6'] == -1) | (df['PAY_6'] == 0)
df.loc[fill, 'PAY_6'] = 0
notpay = df[['PAY_1','PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']]

fill = (df['EDUCATION'] == 5) | (df['EDUCATION'] == 6) | (df['EDUCATION'] == 0)
df.loc[fill, 'EDUCATION'] = 4

correlation('EDUCATION', 'SEX')

# then calulate the correlation after replace some vaules 
correlation('EDUCATION','default_payment')

correlation('MARRIAGE', 'SEX')

df.loc[df['PAY_1'] > 0, 'PAY_1'] = 1
df.loc[df['PAY_2'] > 0, 'PAY_2'] = 1
df.loc[df['PAY_3'] > 0, 'PAY_3'] = 1
df.loc[df['PAY_4'] > 0, 'PAY_4'] = 1
df.loc[df['PAY_5'] > 0, 'PAY_5'] = 1
df.loc[df['PAY_6'] > 0, 'PAY_6'] = 1

df

"""###**Feature Engineering**

"""

df['SEX_MAR'] = df['SEX']*df['MARRIAGE']

correlation('SEX_MAR','default_payment')

df['SEX_MAR']=0
df.loc[((df['SEX']==1) & (df['MARRIAGE']==1)), 'SEX_MAR']=1
df.loc[((df['SEX']==1) & (df['MARRIAGE']==2)), 'SEX_MAR']=2
df.loc[((df['SEX']==1) & (df['MARRIAGE']==3)), 'SEX_MAR']=3
df.loc[((df['SEX']==2) & (df['MARRIAGE']==1)), 'SEX_MAR']=4
df.loc[((df['SEX']==2) & (df['MARRIAGE']==2)), 'SEX_MAR']=5
df.loc[((df['SEX']==2) & (df['MARRIAGE']==3)), 'SEX_MAR']=6

correlation('SEX_MAR','default_payment')

df['AGE']

# df['agebin']=0
# df.loc[((df['AGE']>20)& (df['AGE']<30)),'agebin']=1
# df.loc[((df['AGE']>30)& (df['AGE']<40)),'agebin']=2
# df.loc[((df['AGE']>40)& (df['AGE']<50)),'agebin']=3
# df.loc[((df['AGE']>50)& (df['AGE']<60)),'agebin']=4
# df.loc[((df['AGE']>60)& (df['AGE']<70)),'agebin']=5
# df.loc[((df['AGE']>70)& (df['AGE']<81)),'agebin']=6

bins=[20,29,39,49,59,69,81]
bin_num=[1,2,3,4,5,6]
df['agebin']=pd.cut(df['AGE'],bins,labels=bin_num)

df['agebin'].value_counts().plot(kind = 'bar')

df['agebin1']=pd.qcut(df['AGE'],6)

df['agebin1'].value_counts

df['agebin1']= pd.qcut(df['AGE'],6,labels=bin_num )

df['agebin1'].value_counts().plot(kind = 'bar')

df['agebin']=pd.cut(df['AGE'],6,labels=[1,2,3,4,5,6])
df['agebin']=pd.to_numeric(df['agebin'])
df.loc[(df['agebin']==6),'agebin']=5

df['agebin'].hist()

correlation('agebin','default_payment')

correlation('agebin','SEX')

df['agebin']

df['SEX_AGE']=0
df.loc[((df['SEX']==1)&(df['agebin']==1)),'SEX_AGE']=1
df.loc[((df['SEX']==1)&(df['agebin']==2)),'SEX_AGE']=2
df.loc[((df['SEX']==1)&(df['agebin']==3)),'SEX_AGE']=3
df.loc[((df['SEX']==1)&(df['agebin']==4)),'SEX_AGE']=4
df.loc[((df['SEX']==1)&(df['agebin']==5)),'SEX_AGE']=5
df.loc[((df['SEX']==2)&(df['agebin']==1)),'SEX_AGE']=6
df.loc[((df['SEX']==2)&(df['agebin']==2)),'SEX_AGE']=7
df.loc[((df['SEX']==2)&(df['agebin']==3)),'SEX_AGE']=8
df.loc[((df['SEX']==2)&(df['agebin']==4)),'SEX_AGE']=9
df.loc[((df['SEX']==2)&(df['agebin']==5)),'SEX_AGE']=10

df['SEX_AGE'].value_counts().plot(kind='bar')

correlation('SEX_AGE','default_payment')

""" if PAY, BILL_AMT and PAY_AMT are 0,are not talking about a client"""

df['client1']=1
df['client2']=1
df['client3']=1
df['client4']=1
df['client5']=1
df['client6']=1
df.loc[((df['PAY_1']==0)&(df['BILL_AMT1']==0)&(df['PAY_AMT1']==0)),'client1']=0
df.loc[((df['PAY_2']==0)&(df['BILL_AMT2']==0)&(df['PAY_AMT2']==0)),'client1']=0
df.loc[((df['PAY_3']==0)&(df['BILL_AMT3']==0)&(df['PAY_AMT3']==0)),'client1']=0
df.loc[((df['PAY_4']==0)&(df['BILL_AMT4']==0)&(df['PAY_AMT4']==0)),'client1']=0
df.loc[((df['PAY_5']==0)&(df['BILL_AMT5']==0)&(df['PAY_AMT5']==0)),'client1']=0
df.loc[((df['PAY_6']==0)&(df['BILL_AMT6']==0)&(df['PAY_AMT6']==0)),'client1']=0

pd.Series([df[df['client1']==1]['default_payment'].count(),
           df[df['client2']==1]['default_payment'].count(),
           df[df['client3']==1]['default_payment'].count(),
           df[df['client4']==1]['default_payment'].count(),
           df[df['client5']==1]['default_payment'].count(),
           df[df['client6']==1]['default_payment'].count()],[1,2,3,4,5,6])

"""the expenses of a client"""

df['AvgExp1'] = (((df['BILL_AMT5'] - (df['BILL_AMT6'] - df['PAY_AMT5'])) +
                 (df['BILL_AMT4'] - (df['BILL_AMT5'] - df['PAY_AMT4'])) +
                 (df['BILL_AMT3'] - (df['BILL_AMT4'] - df['PAY_AMT3'])) +
                 (df['BILL_AMT2'] - (df['BILL_AMT3'] - df['PAY_AMT2'])) +
                 (df['BILL_AMT1'] - (df['BILL_AMT2'] - df['PAY_AMT1']))) / 5) / df['LIMIT_BAL']

df['AvgExp2'] = (((df['BILL_AMT5'] - (df['BILL_AMT6'] - df['PAY_AMT5'])) +
                 (df['BILL_AMT4'] - (df['BILL_AMT5'] - df['PAY_AMT4'])) +
                 (df['BILL_AMT3'] - (df['BILL_AMT4'] - df['PAY_AMT3'])) +
                 (df['BILL_AMT2'] - (df['BILL_AMT3'] - df['PAY_AMT2']))) / 4) / df['LIMIT_BAL']

df['AvgExp3'] = (((df['BILL_AMT5'] - (df['BILL_AMT6'] - df['PAY_AMT5'])) +
                 (df['BILL_AMT4'] - (df['BILL_AMT5'] - df['PAY_AMT4'])) +
                 (df['BILL_AMT3'] - (df['BILL_AMT4'] - df['PAY_AMT3']))) / 3) / df['LIMIT_BAL']

df['AvgExp4'] = (((df['BILL_AMT5'] - (df['BILL_AMT6'] - df['PAY_AMT5'])) +
                 (df['BILL_AMT4'] - (df['BILL_AMT5'] - df['PAY_AMT4']))) / 2) / df['LIMIT_BAL']

df['AvgExp5'] = ((df['BILL_AMT5'] - (df['BILL_AMT6'] - df['PAY_AMT5']))) / df['LIMIT_BAL']

df

df['default_payment']

df['C1'] = (df.LIMIT_BAL - df.BILL_AMT1) / df.LIMIT_BAL
df['C2'] = (df.LIMIT_BAL - df.BILL_AMT2) / df.LIMIT_BAL
df['C3'] = (df.LIMIT_BAL - df.BILL_AMT3) / df.LIMIT_BAL
df['C4'] = (df.LIMIT_BAL - df.BILL_AMT4) / df.LIMIT_BAL
df['C5'] = (df.LIMIT_BAL - df.BILL_AMT5) / df.LIMIT_BAL
df['C6'] = (df.LIMIT_BAL - df.BILL_AMT6) / df.LIMIT_BAL

df

count_nan = df.isna().sum().sum()

print ('Count of NaN: ' + str(count_nan))

import pandas as pd
import numpy as np

def clean_dataset(df):
    assert isinstance(df, pd.DataFrame), "df needs to be a pd.DataFrame"
    df.dropna(inplace=True)
    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)
    return df[indices_to_keep].astype(np.float64)

"""#Modeling using Machine Learning"""

from sklearn.utils import resample
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification
from sklearn.metrics import classification_report
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn import metrics
from sklearn.ensemble import AdaBoostClassifier
from sklearn.datasets import make_classification
from sklearn.datasets import make_hastie_10_2
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split,GridSearchCV
from xgboost import XGBClassifier

# X = df.drop('default_payment',1)
# y = df['default_payment']

# x = df.sample(frac=1) # shuffle -> fixes boosting errors
# y = df.iloc[:,[0]] #extract label from last column
# X = df.drop(['default_payment'],axis=1) #drop last column from X

features = ['LIMIT_BAL', 'EDUCATION', 'MARRIAGE', 'PAY_1','PAY_2', 'PAY_3', 
            'PAY_4', 'PAY_5', 'PAY_6','BILL_AMT1', 'BILL_AMT2',
            'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1',
            'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6', 
            'SEX_MAR', 'agebin', 'SEX_AGE', 'AvgExp5', 'AvgExp4',
            'AvgExp3', 'AvgExp2', 'AvgExp1', 'C5',
            'C4', 'C3', 'C2','C1']
y = df['default_payment'].copy() 
X = df[features].copy()
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)

# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)

df_train = X_train.join(y_train)
df_maj = df_train[df_train.default_payment==0]
df_min = df_train[df_train.default_payment==1]

"""training samples"""

#upsample 
df_upsampled = resample(df_maj, replace=True, n_samples=18677, random_state=42) 
df_upsampled = pd.concat([df_maj, df_min])

X_upsampled = df_upsampled.drop(['default_payment'], axis= 1)
y_upsampled = df_upsampled.default_payment

#downsample
df_downsampled = resample(df_min, replace=False,n_samples=5323,random_state=587) 
df_downsampled = pd.concat([df_maj, df_min])

X_downsampled = df_downsampled.drop(['default_payment'], axis = 1)
y_downsampled = df_downsampled.default_payment

sm = SMOTE()
X_SMOTE, y_SMOTE = sm.fit_resample(X_train, y_train)

count_nan = df.isna().sum().sum()

print ('Count of NaN: ' + str(count_nan))

"""###Random Forest"""

#normal train
clf = RandomForestClassifier(max_depth=2, random_state=0)
clf.fit(X_train, y_train)
pred = clf.predict(X_test)
target_names = ['class 0', 'class 1']
print(classification_report(y_test, pred, target_names=target_names))

ConfusionMatrixDisplay(metrics.confusion_matrix(pred,y_test)).plot(cmap='GnBu')
plt.title("Confusion matrix ")
plt.show()

# Upsample training
clf_up = RandomForestClassifier(max_depth=2, random_state=0)
clf_up.fit(X_upsampled, y_upsampled)
pred_up = clf_up.predict(X_test)
print(classification_report(y_test, pred_up, target_names=target_names))

ConfusionMatrixDisplay(metrics.confusion_matrix(pred_up,y_test)).plot(cmap='GnBu')
plt.title("Confusion matrix ")
plt.show()

# Downsample training
clf_down = RandomForestClassifier(max_depth=2, random_state=0)
clf_down.fit(X_downsampled, y_downsampled)
pred_down = clf_down.predict(X_test)
print(classification_report(y_test, pred_down, target_names=target_names))

ConfusionMatrixDisplay(metrics.confusion_matrix(pred_down,y_test)).plot(cmap='GnBu')
plt.title("Confusion matrix ")
plt.show()

# SMOTE training
clf_SMOTE = RandomForestClassifier(max_depth=2, random_state=0)
clf_SMOTE.fit(X_SMOTE,y_SMOTE)
pred_SMOTE = clf_SMOTE.predict(X_test)
print(classification_report(y_test, pred_SMOTE, target_names=target_names))

ConfusionMatrixDisplay(metrics.confusion_matrix(pred_SMOTE,y_test)).plot(cmap='GnBu')
plt.title("Confusion matrix ")
plt.show()

"""##Adaboost"""

#normal train
clf = AdaBoostClassifier(n_estimators=100, random_state=0)
clf.fit(X_train, y_train)
pred = clf.predict(X_test)
print(classification_report(y_test, pred, target_names=target_names))

ConfusionMatrixDisplay(metrics.confusion_matrix(pred,y_test)).plot(cmap='GnBu')
plt.title("Confusion matrix ")
plt.show()

# Upsample training
clf_up = AdaBoostClassifier(n_estimators=100, random_state=0)
clf_up.fit(X_upsampled, y_upsampled)
pred_up = clf_up.predict(X_test)
print(classification_report(y_test, pred_up, target_names=target_names))

ConfusionMatrixDisplay(metrics.confusion_matrix(pred_up,y_test)).plot(cmap='GnBu')
plt.title("Confusion matrix ")
plt.show()

# Downsample training
clf_down = AdaBoostClassifier(n_estimators=100, random_state=0)
clf_down.fit(X_downsampled, y_downsampled)
pred_down = clf_down.predict(X_test)
print(classification_report(y_test, pred_down, target_names=target_names))

ConfusionMatrixDisplay(metrics.confusion_matrix(pred_down,y_test)).plot(cmap='GnBu')
plt.title("Confusion matrix ")
plt.show()

# SMOTE training
clf_SMOTE = AdaBoostClassifier(n_estimators=100, random_state=0)
clf_SMOTE.fit(X_SMOTE, y_SMOTE)
pred_SMOTE = clf_SMOTE.predict(X_test)
print(classification_report(y_test, pred_SMOTE, target_names=target_names))

ConfusionMatrixDisplay(metrics.confusion_matrix(pred_SMOTE,y_test)).plot(cmap='GnBu')
plt.title("Confusion matrix ")
plt.show()

"""##GradientBoosting"""

#normal train
clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,
    max_depth=1, random_state=0).fit(X_train, y_train)
pred = clf.predict(X_test)
print(classification_report(y_test, pred, target_names=target_names))

ConfusionMatrixDisplay(metrics.confusion_matrix(pred,y_test)).plot(cmap='GnBu')
plt.title("Confusion matrix ")
plt.show()

# Upsample training
clf_up = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,
    max_depth=1, random_state=0).fit(X_upsampled, y_upsampled)
pred_up = clf_up.predict(X_test)
print(classification_report(y_test, pred_up, target_names=target_names))

ConfusionMatrixDisplay(metrics.confusion_matrix(pred_up,y_test)).plot(cmap='GnBu')
plt.title("Confusion matrix ")
plt.show()

# Downsample training
clf_down = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,
    max_depth=1, random_state=0).fit(X_downsampled, y_downsampled)
pred_down = clf_down.predict(X_test)
print(classification_report(y_test, pred_down, target_names=target_names))

ConfusionMatrixDisplay(metrics.confusion_matrix(pred_down,y_test)).plot(cmap='GnBu')
plt.title("Confusion matrix ")
plt.show()

# SMOTE training
clf_SMOTE = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,
    max_depth=1, random_state=0).fit(X_SMOTE, y_SMOTE)
pred_SMOTE = clf_SMOTE.predict(X_test)
print(classification_report(y_test, pred_SMOTE, target_names=target_names))

ConfusionMatrixDisplay(metrics.confusion_matrix(pred_SMOTE,y_test)).plot(cmap='GnBu')
plt.title("Confusion matrix ")
plt.show()





"""# **LightGBM Classifier**

##Normal sample training
"""

import lightgbm as lgb
from sklearn.metrics import accuracy_score

clf = lgb.LGBMClassifier()
clf.fit(X_train, y_train)
# predict the results
y_pred=clf.predict(X_test)
# view accuracy
accuracy=accuracy_score(y_pred, y_test)
print('LightGBM Model accuracy score: {0:0.4f}'.format(accuracy_score(y_test, y_pred)))

y_pred_train = clf.predict(X_train)
print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))

# print the scores on training and test set

print('Training set score: {:.4f}'.format(clf.score(X_train, y_train)))

print('Test set score: {:.4f}'.format(clf.score(X_test, y_test)))

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))



"""##Upsample training"""

clf2 = lgb.LGBMClassifier()
clf2.fit(X_upsampled, y_upsampled)

# predict the results
y_pred_2=clf2.predict(X_test)
# view accuracy
accuracy_up=accuracy_score(y_pred_2, y_test)
print('LightGBM Model accuracy score: {0:0.4f}'.format(accuracy_score(y_test, y_pred_2)))

y_pred_train_2 = clf2.predict(X_upsampled)
print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_upsampled, y_pred_train_2)))

# print the scores on training and test set

print('Training set score: {:.4f}'.format(clf.score(X_upsampled, y_upsampled)))

print('Test set score: {:.4f}'.format(clf.score(X_test, y_test)))

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred_2))

"""##downsample training"""

clf3 = lgb.LGBMClassifier()
clf3.fit(X_downsampled, y_downsampled)
# predict the results
y_pred_3=clf3.predict(X_test)
# view accuracy
accuracy_up=accuracy_score(y_pred_3, y_test)
print('LightGBM Model accuracy score: {0:0.4f}'.format(accuracy_score(y_test, y_pred_3)))

y_pred_train_3 = clf3.predict(X_downsampled)
print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_downsampled, y_pred_train_3)))

# print the scores on training and test set

print('Training set score: {:.4f}'.format(clf3.score(X_downsampled, y_downsampled)))

print('Test set score: {:.4f}'.format(clf3.score(X_test, y_test)))

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred_3))

"""##SMOTE training"""

clf4 = lgb.LGBMClassifier()
clf4.fit(X_SMOTE, y_SMOTE)
# predict the results
y_pred_4=clf4.predict(X_test)
# view accuracy
accuracy_up=accuracy_score(y_pred_4, y_test)
print('LightGBM Model accuracy score: {0:0.4f}'.format(accuracy_score(y_test, y_pred_4)))

y_pred_train_4 = clf4.predict(X_SMOTE)
print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_SMOTE, y_pred_train_4)))

# print the scores on training and test set

print('Training set score: {:.4f}'.format(clf4.score(X_SMOTE, y_SMOTE)))

print('Test set score: {:.4f}'.format(clf4.score(X_test, y_test)))

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred_4))



"""# GridSearch"""

parm_grid = {'max_depth': [7,11],'learning_rate':[0.2,0.5],'eval_metric':['mlogloss'] }

xgb = XGBClassifier()
grid = GridSearchCV(estimator=xgb, param_grid=parm_grid , cv = 10 )
grid_result_xgb_1 =grid.fit(X_train, y_train)

print(grid_result_xgb_1.best_score_, grid_result_xgb_1.best_params_)

model1=grid_result_xgb_1.best_estimator_

y_pred_xgb_1=model1.predict(X_test)
accuracy_score(y_test,y_pred_xgb_1)

print(classification_report(y_test, y_pred_xgb_1))

"""upsample"""

xgb = XGBClassifier()
grid = GridSearchCV(estimator=xgb, param_grid=parm_grid , cv = 10 )
grid_result_xgb_2 =grid.fit(X_upsampled, y_upsampled)
print(grid_result_xgb_2.best_score_, grid_result_xgb_2.best_params_)

model2=grid_result_xgb_2.best_estimator_
y_pred_xgb_2=model2.predict(X_test)
accuracy_score(y_test,y_pred_xgb_2)

print(classification_report(y_test, y_pred_xgb_2))

"""downsampled"""

xgb = XGBClassifier()
grid = GridSearchCV(estimator=xgb, param_grid=parm_grid , cv = 10 )
grid_result_xgb_3 =grid.fit(X_downsampled, y_downsampled)
print(grid_result_xgb_3.best_score_, grid_result_xgb_3.best_params_)

model3=grid_result_xgb_3.best_estimator_
y_pred_xgb_3=model3.predict(X_test)
accuracy_score(y_test,y_pred_xgb_3)

print(classification_report(y_test, y_pred_xgb_3))

"""SMOTE"""

xgb = XGBClassifier()
grid = GridSearchCV(estimator=xgb, param_grid=parm_grid , cv = 10 )
grid_result_xgb_4 =grid.fit(X_SMOTE, y_SMOTE)
print(grid_result_xgb_4.best_score_, grid_result_xgb_4.best_params_)

model4=grid_result_xgb_4.best_estimator_
y_pred_xgb_4=model4.predict(X_test)
accuracy_score(y_test,y_pred_xgb_4)

print(classification_report(y_test, y_pred_xgb_4))

"""#pycaret"""

pip install pycaret

from pycaret.datasets import get_data

dataset = get_data("credit")

dataset.shape

dataset.columns

data = dataset.sample(frac = 0.95,random_state=0)

data.shape

#solution a problem of random index
data.reset_index(inplace = True,drop = True)

data.reset_index(inplace = True,drop = True)

print("Data for Modeling:" + str(data.shape))
print("Unseen_data for Modeling:" + str(data.shape))

pip install markupsafe==2.0.1

! pip install numba --upgrade

#setup the enviroment
import jinja2
from pycaret.classification import *

clf01 = setup(data=data , target = "default",session_id= 123)

best_model = compare_models()

print(best_model)

models()

dt = create_model("dt")

knn = create_model("knn")

rf = create_model("rf")

tune_rf = tune_model(rf)

unseen_pred = predict_model(tune_rf,data = data)









